<!DOCTYPE html>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Synthesizing Images on Perceptual Boundaries (BAM)</title>
  <link rel="stylesheet" href="css/style.css" />
</head>
<body>
  <header class="site-header">
    <h1>Synthesizing Images on Perceptual Boundaries of ANNs</h1>
    <p class="subtitle">
      Uncovering and Manipulating Human Perceptual Variability with the BAM Framework
    </p>
    <!-- NEW: Authors -->
    <p class="authors">
      <strong>Authors:</strong>
      <a href="https://scholar.google.com/citations?hl=en&user=9-MdqPkAAAAJ" target="_blank">Chen Wei<sup>*</sup></a>,
      <a href="https://eaterminator.github.io/" target="_blank">Chi Zhang<sup>*</sup></a>,
      <a href="https://phyever.github.io/" target="_blank">Jiachen Zou</a>,
      <a href="https://openreview.net/profile?id=~Haotian_Deng2" target="_blank">Haotian Deng</a>,
      <a href="https://www.birmingham.ac.uk/staff/profiles/psychology/heinke-dietmar" target="_blank">Dietmar Heinke</a>,
      <a href="https://scholar.google.com/citations?user=UpP9hJ8AAAAJ&hl=en" target="_blank">Quanying Liu<sup>&dagger;</sup></a>
    </p>
  </header>


  <main class="content">
    <!-- Animated GIF hero section -->
    <section class="hero">
      <img src="assets/gif_fig2.gif" alt="BAM framework demo" class="hero-image" />
      <p class="caption">Overview of BAM: the Perceptual Boundary Alignment & Manipulation Framework</p>
    </section>

    <!-- Project overview -->
    <section class="overview">
      <h2>Abstract</h2>
      Human decision-making in cognitive tasks and daily life exhibits considerable variability, shaped by factors such as task difficulty, individual preferences, and personal experiences. 
      Understanding this variability across individuals is essential for uncovering the perceptual and decision-making mechanisms that humans rely on when faced with uncertainty and ambiguity. 
      We propose a systematic Boundary Alignment Manipulation (BAM) framework for studying human perceptual variability through image generation. BAM combines perceptual boundary sampling in ANNs and human behavioral experiments to systematically investigate this phenomenon. 
      Our perceptual boundary sampling algorithm generates stimuli along ANN perceptual boundaries that intrinsically induce significant perceptual variability. 
      The efficacy of these stimuli is empirically validated through large-scale behavioral experiments involving 246 participants across 116,715 trials, culminating in the variMNIST dataset containing 19,943 systematically annotated images.
      Through personalized model alignment and adversarial generation, we establish a reliable method for simultaneously predicting and manipulating the divergent perceptual decisions of pairs of participants.
      This work bridges the gap between computational models and human individual difference research, providing new tools for personalized perception analysis.
    </section>
    <!-- Perceptual Boundary Sampling Algorithm -->
    <section class="overview">
      <h2>Perceptual Boundary Sampling Algorithm</h2>
       <figure class="algorithm-figure">
        <img
          src="assets/fig3_icml_new.png"
          alt="Flowchart of the Perceptual Boundary Sampling Algorithm"
          class="algorithm-image"
        />
        <figcaption>
          <p class="caption">Flowchart of our Perceptual Boundary Sampling Algorithm.  
          Samples are drawn along an ANN’s decision boundary (high-uncertainty regions), then passed to human observers to measure perceptual ambiguity.</p>
        </figcaption>
      </figure>
      The image perturbations that significantly affect ANN perception also influence human perception, suggesting that ANNs and humans may share similar perceptual boundaries. 
      Based on this, we hypothesize that samples on these boundaries (which exhibit high perceptual variability for ANNs) may also lead to ambiguous perception in humans, resulting in different internal experiences for the same stimuli.
    </section>
    
    <!-- Collecting Human perceptual variability -->
    <section class="overview">
      <h2>Collecting Human Perceptual Variability</h2>
       <figure class="algorithm-figure">
        <img
          src="assets/fig4_icml.png"
          alt="Guidance Outcome."
          class="algorithm-image"
        />
        <figcaption>
          <p class="caption">Guidance Outcome. (a) shows the definition for guidance outcomes, (b) shows the guidance outcome of the hand-written digits and natural images.</p>
        </figcaption>
      </figure>
        <p>
          To comprehensively evaluate the guiding effectiveness of the generation method, we define three types of <em>guidance outcome</em>: <em>success</em>, <em>bias</em>, and <em>failure</em>. 
          For the guidance targets <em>o<sub>1</sub></em> and <em>o<sub>2</sub></em>, let <em>p<sub>1</sub></em> and <em>p<sub>2</sub></em> represent the probabilities of participants choosing <em>o<sub>1</sub></em> and <em>o<sub>2</sub></em>, respectively.
        </p>
        <p>
          A result is considered <em>success</em> if 
          <span class="math">\(p_1 + p_2 \geq 80\%\)</span> 
          and 
          <span class="math">\(\min(p_1, p_2) \geq 10\%\)</span>, 
          indicating the generated stimuli guide participants to make a balanced choice between the two targets.
        </p>
        <p>
          A result is labeled as <em>bias</em> if 
          <span class="math">\(p_1 + p_2 \geq 80\%\)</span> 
          but 
          <span class="math">\(\min(p_1, p_2) < 10\%\)</span>, 
          indicating a strong bias toward one target.
        </p>
        <p>
          A result is classified as <em>failure</em> if 
          <span class="math">\(p_1 + p_2 < 80\%\)</span>, 
          meaning the stimuli fail to guide participants effectively.
        </p>
        <p>
          These definitions allow us to evaluate and compare the performance of different guidance strategies and classifiers.
        </p>
    </section>
    <section class="overview">
      
    </section>
    <!-- GitHub link -->
    <section class="links">
      <h2>Code Repository</h2>
      <p>
        View the full source code on GitHub:  
        <a href="https://github.com/your-username/your-repo" target="_blank">
          https://github.com/your-username/your-repo
        </a>
      </p>
    </section>
  </main>

  <footer class="site-footer">
    <p>© 2025 Synthesizing Images on Perceptual Boundaries of ANNs</p>
  </footer>
</body>
</html>
