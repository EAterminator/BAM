<!DOCTYPE html>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Synthesizing Images on Perceptual Boundaries (BAM)</title>
  <link rel="stylesheet" href="css/style.css" />
</head>
<body>
  <header class="site-header">
    <h1>Synthesizing Images on Perceptual Boundaries of ANNs</h1>
    <p class="subtitle">
      Uncovering and Manipulating Human Perceptual Variability with the BAM Framework
    </p>
    <!-- NEW: Authors -->
    <p class="authors">
      <strong>Authors:</strong>
      <a href="https://scholar.google.com/citations?hl=en&user=9-MdqPkAAAAJ" target="_blank">Chen Wei<sup>*</sup></a>,
      <a href="https://eaterminator.github.io/" target="_blank">Chi Zhang<sup>*</sup></a>,
      <a href="https://phyever.github.io/" target="_blank">Jiachen Zou</a>,
      <a href="https://openreview.net/profile?id=~Haotian_Deng2" target="_blank">Haotian Deng</a>,
      <a href="https://www.birmingham.ac.uk/staff/profiles/psychology/heinke-dietmar" target="_blank">Dietmar Heinke</a>,
      <a href="https://scholar.google.com/citations?user=UpP9hJ8AAAAJ&hl=en" target="_blank">Quanying Liu<sup>&dagger;</sup></a>
    </p>
  </header>


  <main class="content">
    <!-- Animated GIF hero section -->
    <section class="hero">
      <img src="assets/gif_fig2.gif" alt="BAM framework demo" class="hero-image" />
      <p class="caption">Overview of BAM: the Perceptual Boundary Alignment & Manipulation Framework</p>
    </section>

    <!-- Project overview -->
    <section class="overview">
      <h2>Overview</h2>
      Human decision-making in cognitive tasks and daily life exhibits considerable variability, shaped by factors such as task difficulty, individual preferences, and personal experiences. 
      Understanding this variability across individuals is essential for uncovering the perceptual and decision-making mechanisms that humans rely on when faced with uncertainty and ambiguity. 
      We propose a systematic Boundary Alignment Manipulation (BAM) framework for studying human perceptual variability through image generation. BAM combines perceptual boundary sampling in ANNs and human behavioral experiments to systematically investigate this phenomenon. 
      Our perceptual boundary sampling algorithm generates stimuli along ANN perceptual boundaries that intrinsically induce significant perceptual variability. 
      The efficacy of these stimuli is empirically validated through large-scale behavioral experiments involving 246 participants across 116,715 trials, culminating in the variMNIST dataset containing 19,943 systematically annotated images.
      Through personalized model alignment and adversarial generation, we establish a reliable method for simultaneously predicting and manipulating the divergent perceptual decisions of pairs of participants.
      This work bridges the gap between computational models and human individual difference research, providing new tools for personalized perception analysis.
    </section>
    <!-- Perceptual Boundary Sampling Algorithm -->
    <section class="overview">
      <h2>Perceptual Boundary Sampling Algorithm</h2>
       <figure class="algorithm-figure">
        <img
          src="assets/fig3_icml_new.png"
          alt="Flowchart of the Perceptual Boundary Sampling Algorithm"
          class="algorithm-image"
        />
        <figcaption>
          <p class="caption">Flowchart of our Perceptual Boundary Sampling Algorithm.  
          Samples are drawn along an ANN’s decision boundary (high-uncertainty regions), then passed to human observers to measure perceptual ambiguity.</p>
        </figcaption>
      </figure>
      The image perturbations that significantly affect ANN perception also influence human perception, suggesting that ANNs and humans may share similar perceptual boundaries. 
      Based on this, we hypothesize that samples on these boundaries (which exhibit high perceptual variability for ANNs) may also lead to ambiguous perception in humans, resulting in different internal experiences for the same stimuli.
    </section>
    
    <!-- Collecting Human perceptual variability -->
    <section class="overview">
      <h2>Collecting Human Perceptual Variability</h2>
       <figure class="algorithm-figure">
        <img
          src="assets/fig4_icml.png"
          alt="Guidance Outcome."
          class="algorithm-image"
        />
        <figcaption>
          <p class="caption">Guidance Outcome. (a) shows the definition for guidance outcomes, (b) shows the guidance outcome of the hand-written digits and natural images.</p>
        </figcaption>
      </figure>
        <p>
          To comprehensively evaluate the guiding effectiveness of the generation method, we define three types of <em>guidance outcome</em>: <em>success</em>, <em>bias</em>, and <em>failure</em>. 
          For the guidance targets <em>o<sub>1</sub></em> and <em>o<sub>2</sub></em>, let <em>p<sub>1</sub></em> and <em>p<sub>2</sub></em> represent the probabilities of participants choosing <em>o<sub>1</sub></em> and <em>o<sub>2</sub></em>, respectively.
        </p>
        <p>
          A result is considered <em>success</em> if 
          <span class="math">\(p_1 + p_2 \geq 80\%\)</span> 
          and 
          <span class="math">\(\min(p_1, p_2) \geq 10\%\)</span>, 
          indicating the generated stimuli guide participants to make a balanced choice between the two targets.
        </p>
        <p>
          A result is labeled as <em>bias</em> if 
          <span class="math">\(p_1 + p_2 \geq 80\%\)</span> 
          but 
          <span class="math">\(\min(p_1, p_2) < 10\%\)</span>, 
          indicating a strong bias toward one target.
        </p>
        <p>
          A result is classified as <em>failure</em> if 
          <span class="math">\(p_1 + p_2 < 80\%\)</span>, 
          meaning the stimuli fail to guide participants effectively.
        </p>
        <p>
          These definitions allow us to evaluate and compare the performance of different guidance strategies and classifiers.
        </p>
    </section>
    <section class="overview">
      <h2>Predicting Human Perceptual Variability</h2>
      <figure class="algorithm-figure">
        <img
          src="assets/fig5_icml.png"
          alt="Results of predicting human perceptual variability"
          class="algorithm-image"
        />
        <figcaption>
          <p class="caption">Human Alignment Results.
          <p>
            (a) Accuracy of BaseNet, GroupNet, and IndivNet on MNIST, variMNIST, and variMNIST-i.  
            All models performed similarly on MNIST. On variMNIST, GroupNet and IndivNet improved accuracy by 
            <span class="math">\( \sim 20\%\)</span> over BaseNet, with IndivNet outperforming GroupNet by 
            <span class="math">\( \sim 5\%\)</span> on variMNIST-i. Accuracy improved for 241 participants and decreased for 5 after individual fine-tuning.
          </p>
          
          <p>
            (b) Fine-tuning results for five classifiers used in the work.
          </p>
          
          <p>
            (c) For VGG, Spearman rank correlation between model and human entropy increased from 
            <span class="math">\( \rho = 0.08\)</span> to <span class="math">\( \rho = 0.74\)</span> after group fine-tuning.
          </p>
          
          <p>
            (d) Performance of BaseNet, GroupNet, and IndivNet across varying entropy levels.  
            The choices from the selected subject for the example images are 8, 6, 9, 6, with increasing entropy.  
            Gray backgrounds indicate model–subject disagreement.  
            GroupNet and IndivNet improved over BaseNet at all entropy levels, while IndivNet’s gains over GroupNet were concentrated on high-entropy images.
          </p></p>
        </figcaption>
      </figure>
      <p>
        To align the base models pretrained on MNIST with the performance of both group and individual levels, we adopted a 2-stage fine-tuning approach.
        For the whole procedure, the group model (GroupNet) was finetuned from the base model (BaseNet), and the individual model (IndivNet) was finetuned from the group model.
        For individual-level datasets (variMNIST-i), which are subsets of variMNIST corresponding to specific individuals, the validation set was designed to avoid overlap with the group validation set. More details can be found in the paper.
      </p>
    </section>
    <section class="overview">
      <h2>Manipulating Human Perceptual Variablity</h2>
      <p>
        Building on variMNIST and alignment experiments, we designed a paradigm to test whether individually fine-tuned models can amplify perceptual differences and guide decision-making. 
        This experiment evaluates the ability of targeted stimuli to reveal individual variability and achieve precise manipulation of perceptual outcomes, highlighting the potential of personalized modeling in understanding human perception.
      </p>
    
      <h3>First Round</h3>
      <p>
        For the <em>first round</em> of experiments, we initially selected around 500 balanced samples from the variMNIST dataset as stimuli. 
        After collecting behavioral data from pairs of participants, we fine-tuned their individual models. 
        Controversial stimuli were then generated using the updated models, aiming to elicit distinct choices between the two participants, with each choosing their respective guidance targets.
      </p>
    
      <h3>Second Round</h3>
      <p>
        In the <em>second round</em> of experiments, these controversial stimuli were presented to participants in pairs, with each pair completing trials designed to test whether the fine-tuned models could effectively guide their decisions in opposite directions. 
        The goal was to evaluate whether the generated stimuli amplified perceptual differences and aligned participants’ responses with their respective guidance targets. Experiment details can be found in the paper.
      </p>
       <figure class="algorithm-figure">
        <img
          src="assets/fig6_icml.png"
          alt="Flowchart of the Perceptual Boundary Sampling Algorithm"
          class="algorithm-image"
        />
        <figcaption>
          <p class="caption"> Manipulation Analysis.
            <p> 
              <strong>(a)</strong> Manipulation experiment procedure.
            </p>
          
            <p>
              <strong>(b)</strong> The middle two bars show the guidance outcomes for variMNIST and the individually customized dataset, with the individually customized dataset achieving a higher success rate. Compared to variMNIST, IndivNets also improve the directionality of guiding perceptual changes.
            </p>
          
            <p>
              <strong>(c)</strong> The left panel shows the guidance success rates for the first-round stimuli and the second-round stimuli generated by the fine-tuned models, with an improvement of <span class="math">\(\sim 3\%\)</span> (<span class="math">\(p < 0.001\)</span>). The right panel shows the <em>targeted ratios</em> (i.e., the proportion of participant choices aligned with the guidance direction) for these two groups of stimuli, with an increase of <span class="math">\(\sim 12\%\)</span> (<span class="math">\(p < 0.001\)</span>).
            </p>
          </p>
        </figcaption>
      </figure>
        <p>
          To analyze the effects of individual manipulation, we employed two key metrics. The first metric, referred to as the 
          <strong>guidance outcome</strong>, was adapted from the <strong>Predicting Human Perceptual Variability</strong> section. 
          It categorizes outcomes for two participants, <span class="math">\(s_1\)</span> and <span class="math">\(s_2\)</span>, with respective guidance targets 
          <span class="math">\(o_1\)</span> and <span class="math">\(o_2\)</span>, and choices <span class="math">\(c_1\)</span> and <span class="math">\(c_2\)</span>. 
          A result is labeled as <em>success</em> if both participants’ choices fall within their respective guidance targets and are distinct, i.e., 
          <span class="math">\(c_1, c_2 \in \{o_1, o_2\}\)</span> and <span class="math">\(c_1 \neq c_2\)</span>. 
          If both choices are biased toward the same target, such as <span class="math">\(c_1 = c_2 = o_1\)</span> or <span class="math">\(c_1 = c_2 = o_2\)</span>, 
          it is categorized as <em>bias</em>. 
          Finally, if at least one choice is outside the targets 
          <span class="math">\(c_1, c_2 \notin \{o_1, o_2\}\)</span>, the outcome is labeled as <em>failure</em>.
        </p>
      
        <p>
          The second metric, called the <strong>targeted ratio</strong>, quantifies the directionality of successful guidance. 
          Within successful trials, participant choices are classified as either <em>positive</em>, where 
          <span class="math">\(c_1 = o_1\)</span> and <span class="math">\(c_2 = o_2\)</span>, meaning both choices align with their respective targets, 
          or <em>negative</em>, where <span class="math">\(c_1 = o_2\)</span> and <span class="math">\(c_2 = o_1\)</span>, indicating swapped choices. 
          The targeted ratio is defined as the proportion of positive trials among all success trials, providing a measure of the effectiveness of directional guidance.
        </p>
    </section>
    
    <!-- GitHub link -->
    <section class="links">
      <h2>Code Repository</h2>
      
      <p>
        View the full source code on GitHub:  
        <a href="https://github.com/your-username/your-repo" target="_blank">
          https://github.com/your-username/your-repo
        </a>
      </p>
    </section>
  </main>

  <footer class="site-footer">
    <p>© 2025 Synthesizing Images on Perceptual Boundaries of ANNs</p>
  </footer>
</body>
</html>
