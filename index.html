<!DOCTYPE html>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Synthesizing Images on Perceptual Boundaries (BAM)</title>
  <link rel="stylesheet" href="css/style.css" />
</head>
<body>
  <header class="site-header">
    <h1>Synthesizing Images on Perceptual Boundaries of ANNs</h1>
    <p class="subtitle">
      Uncovering and Manipulating Human Perceptual Variability with the BAM Framework
    </p>
    <!-- NEW: Authors -->
    <p class="authors">
      <strong>Authors:</strong>
      <a href="https://scholar.google.com/citations?hl=en&user=9-MdqPkAAAAJ" target="_blank">Chen Wei<sup>*</sup></a>,
      <a href="https://eaterminator.github.io/" target="_blank">Chi Zhang<sup>*</sup></a>,
      <a href="https://phyever.github.io/" target="_blank">Jiachen Zou</a>,
      <a href="https://openreview.net/profile?id=~Haotian_Deng2" target="_blank">Haotian Deng</a>,
      <a href="https://www.birmingham.ac.uk/staff/profiles/psychology/heinke-dietmar" target="_blank">Dietmar Heinke</a>,
      <a href="https://scholar.google.com/citations?user=UpP9hJ8AAAAJ&hl=en" target="_blank">Quanying Liu<sup>&dagger;</sup></a>
    </p>
  </header>


  <main class="content">
    <!-- Animated GIF hero section -->
    <section class="hero">
      <img src="assets/gif_fig2.gif" alt="BAM framework demo" class="hero-image" />
      <p class="caption">Overview of BAM: the Perceptual Boundary Alignment & Manipulation Framework</p>
    </section>

    <!-- Project overview -->
    <section class="overview">
      <h2>Abstract</h2>
      Human decision-making in cognitive tasks and daily life exhibits considerable variability, shaped by factors such as task difficulty, individual preferences, and personal experiences. 
      Understanding this variability across individuals is essential for uncovering the perceptual and decision-making mechanisms that humans rely on when faced with uncertainty and ambiguity. 
      We propose a systematic Boundary Alignment Manipulation (BAM) framework for studying human perceptual variability through image generation. BAM combines perceptual boundary sampling in ANNs and human behavioral experiments to systematically investigate this phenomenon. 
      Our perceptual boundary sampling algorithm generates stimuli along ANN perceptual boundaries that intrinsically induce significant perceptual variability. 
      The efficacy of these stimuli is empirically validated through large-scale behavioral experiments involving 246 participants across 116,715 trials, culminating in the variMNIST dataset containing 19,943 systematically annotated images.
      Through personalized model alignment and adversarial generation, we establish a reliable method for simultaneously predicting and manipulating the divergent perceptual decisions of pairs of participants.
      This work bridges the gap between computational models and human individual difference research, providing new tools for personalized perception analysis.
    </section>
    <!-- Perceptual Boundary Sampling Algorithm -->
    <section class="overview">
      <h2>Perceptual Boundary Sampling Algorithm</h2>
       <figure class="algorithm-figure">
        <img
          src="assets/fig3_icml_new.png"
          alt="Flowchart of the Perceptual Boundary Sampling Algorithm"
          class="algorithm-image"
        />
        <figcaption>
          <p class="caption">Flowchart of our Perceptual Boundary Sampling Algorithm.  
          Samples are drawn along an ANN’s decision boundary (high-uncertainty regions), then passed to human observers to measure perceptual ambiguity.</p>
        </figcaption>
      </figure>
      The image perturbations that significantly affect ANN perception also influence human perception, suggesting that ANNs and humans may share similar perceptual boundaries. 
      Based on this, we hypothesize that samples on these boundaries (which exhibit high perceptual variability for ANNs) may also lead to ambiguous perception in humans, resulting in different internal experiences for the same stimuli.
    </section>
    
    <!-- Collecting Human perceptual variability -->
    <section class="overview">
      <h2>Collecting Human Perceptual Variability</h2>
       <figure class="algorithm-figure">
        <img
          src="assets/fig4_icml.png"
          alt="Guidance Outcome."
          class="algorithm-image"
        />
        <figcaption>
          <p class="caption">Guidance Outcome. (a) shows the definition for guidance outcomes, (b) shows the guidance outcome of the hand-written digits and natural images.</p>
        </figcaption>
      </figure>
        <p>
          To comprehensively evaluate the guiding effectiveness of the generation method, we define three types of <em>guidance outcome</em>: <em>success</em>, <em>bias</em>, and <em>failure</em>. 
          For the guidance targets <em>o<sub>1</sub></em> and <em>o<sub>2</sub></em>, let <em>p<sub>1</sub></em> and <em>p<sub>2</sub></em> represent the probabilities of participants choosing <em>o<sub>1</sub></em> and <em>o<sub>2</sub></em>, respectively.
        </p>
        <p>
          A result is considered <em>success</em> if 
          <span class="math">\(p_1 + p_2 \geq 80\%\)</span> 
          and 
          <span class="math">\(\min(p_1, p_2) \geq 10\%\)</span>, 
          indicating the generated stimuli guide participants to make a balanced choice between the two targets.
        </p>
        <p>
          A result is labeled as <em>bias</em> if 
          <span class="math">\(p_1 + p_2 \geq 80\%\)</span> 
          but 
          <span class="math">\(\min(p_1, p_2) < 10\%\)</span>, 
          indicating a strong bias toward one target.
        </p>
        <p>
          A result is classified as <em>failure</em> if 
          <span class="math">\(p_1 + p_2 < 80\%\)</span>, 
          meaning the stimuli fail to guide participants effectively.
        </p>
        <p>
          These definitions allow us to evaluate and compare the performance of different guidance strategies and classifiers.
        </p>
    </section>
    <section class="overview">
      <h2>Predicting Human Perceptual Variability</h2>
      <p>
        To align the base models pretrained on MNIST with the performance of both group and individual levels, we adopted a mixed fine-tuning approach with an 80:20 split for training and validation. 
        For individual-level datasets (variMNIST-i), which are subsets of variMNIST corresponding to specific individuals, the validation set was designed to avoid overlap with the group validation set.
        For group-level fine-tuning, we combined the MNIST and variMNIST datasets in a 1:1 ratio, ensuring performance on MNIST while fine-tuning for perceptual variability. 
        For individual-level fine-tuning, we mixed variMNIST-i, variMNIST, and MNIST datasets in a 2:1:1 ratio, ensuring the models performed effectively on individual-specific, group, and original datasets. For each individual, a customized individual model was trained. 
        For the whole procedure, the group model (GroupNet) was finetuned from the base model (BaseNet), and the individual model (IndivNet) was finetuned from the group model.
      </p>
    </section>
    <!-- GitHub link -->
    <section class="links">
      <h2>Code Repository</h2>
      <figure class="algorithm-figure">
        <img
          src="assets/fig5_icml.png"
          alt="Results of predicting human perceptual variability"
          class="algorithm-image"
        />
        <figcaption>
          <p class="caption">.<p> Human Alignment Results.
            (a) Accuracy of BaseNet, GroupNet, and IndivNet on MNIST, variMNIST, and variMNIST-i.  
            All models performed similarly on MNIST. On variMNIST, GroupNet and IndivNet improved accuracy by 
            <span class="math">\( \sim 20\%\)</span> over BaseNet, with IndivNet outperforming GroupNet by 
            <span class="math">\( \sim 5\%\)</span> on variMNIST-i. Accuracy improved for 241 participants and decreased for 5 after individual fine-tuning.
          </p>
          
          <p>
            (b) Fine-tuning results for five classifiers. On MNIST, group fine-tuning improved ViT and VGG, while others remained unchanged or declined.  
            On variMNIST, all classifiers improved, with ViT and MLP showing the largest gains and LRM the smallest.  
            Individual fine-tuning further improved all classifiers following the same trend.
          </p>
          
          <p>
            (c) For VGG, Spearman rank correlation between model and human entropy increased from 
            <span class="math">\( \rho = 0.08\)</span> to <span class="math">\( \rho = 0.74\)</span> after group fine-tuning.
          </p>
          
          <p>
            (d) Performance of BaseNet, GroupNet, and IndivNet across varying entropy levels.  
            The choices from the selected subject for the example images are 8, 6, 9, 6, with increasing entropy.  
            Gray backgrounds indicate model–subject disagreement.  
            GroupNet and IndivNet improved over BaseNet at all entropy levels, while IndivNet’s gains over GroupNet were concentrated on high-entropy images.
          </p></p>
        </figcaption>
      </figure>
      <p>
        View the full source code on GitHub:  
        <a href="https://github.com/your-username/your-repo" target="_blank">
          https://github.com/your-username/your-repo
        </a>
      </p>
    </section>
  </main>

  <footer class="site-footer">
    <p>© 2025 Synthesizing Images on Perceptual Boundaries of ANNs</p>
  </footer>
</body>
</html>
